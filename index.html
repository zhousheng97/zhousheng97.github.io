

<html lang="en">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Sheng Zhou</title>
  
  <meta name="author" content="Sheng Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/zhousheng.jpg">
</head>


<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sheng Zhou (å‘¨æ™Ÿ)</name>
              </p>
              
              <p>
                I am a Ph.D. student in Computer Science at
                <a href="https://ci.hfut.edu.cn/">Hefei University of Technology (HFUT)</a>, advised by  by Prof. <a href="https://faculty.hfut.edu.cn/gd/zh_CN/index.htm">Dan Guo</a> and Prof. <a href="https://faculty.hfut.edu.cn/wm12/zh_CN/index.htm">Meng Wang</a>.
                <!--Previously, I studied at <a href="https://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a> for one year under the guidance of Prof. <a href="https://faculty.ustc.edu.cn/yangxun/en/index.htm">Xun Yang</a>.-->
                Currently, I am studying at <a href="https://www.comp.nus.edu.sg/">National University of Singapore (NUS)</a> as a visiting student under the guidance of Dr. <a href="https://doc-doc.github.io/cv/">Junbin Xiao</a>, Prof. <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>, and Prof. <a href="https://www.comp.nus.edu.sg/cs/people/chuats/">Tat-Seng Chua</a>.
                </p> 

              <p>
                <strong style="color: red;">I am actively seeking research discussions and collaboration opportunities, so feel free to contact me!</strong>
              </p>
              
              <p style="text-align:center">
                <a href="mailto:hzgn97@gmail.com"> [Email] </a>
                <a href="https://github.com/zhousheng97"> [Github] </a> 
                <a href="https://scholar.google.com/citations?user=7r9ejvcAAAAJ&hl=zh-CN"> [Google Scholar] </a>                
              </p>
              
            </td>
             <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:90%;max-width:90%" alt="profile photo" src="images/zhousheng.jpg">
            </td> 
          </tr>
        </tbody></table>

        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
              My research focuses on visual-language understanding and reasoning, 
              primarily on scene-text visual question answering. 
              I am currently expanding my research scope to egocentric video understanding and multimodal large language models for human assistance.
              </p>
          </td>
        </tr>
      </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>ðŸ”¥News</heading>
            <p>
  
              <li style="margin: 5px;" >
                2025.02: <i>One Paper</i> is accepted by <strong>CVPR</strong>. ðŸŽ‰
              </li>
              <li style="margin: 5px;" >
                2025.02: I honor Tat-Seng Chua Scholarship.
              </li>
              <li style="margin: 5px;" >
                2024.09: I will be a visiting student at NUS for one year, collaborating with Dr. <a href="https://doc-doc.github.io/cv/">Junbin Xiao</a>.
              </li>
              <li style="margin: 5px;" >
                2024.01: <i>One paper</i> is accepted by <strong>TOMM</strong>. 
              </li>
              <!--<li style="margin: 5px;" >
                2023.07: Start a study at USTC and supervised by Prof. <a href="https://faculty.ustc.edu.cn/yangxun/en/index.htm">Xun Yang</a>. 
              </li>-->
              <li style="margin: 5px;" >
                2023.09: <i>One paper</i> is accepted by <strong>TIP</strong>. 
              </li>
              
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications and Preprints</heading></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:280;height:130" src="images/egotextvqa.png" alt="egotextvqa.png">
            </td>
            <td width="70%" valign="center" style="line-height: 1.5;">
              <papertitle>EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering</papertitle>
              <br>
              <strong>Sheng Zhou</strong>, 
                Junbin Xiao, 
                Qingyun Li, 
                Yicong Li, 
                Xun Yang, 
                Dan Guo, 
                Meng Wang, 
                Tat-Seng Chua, 
                Angela Yao.
              <br>
              <strong style="color: red;">CVPR'25</strong> 
              <a href="https://arxiv.org/abs/2502.07411">[arXiv]</a>
              <a href="https://zhousheng97.github.io/EgoTextVQA_page/">[Project Page]</a>
              <a href="https://github.com/zhousheng97/EgoTextVQA">[Code]</a>
              <a href="https://github.com/zhousheng97/EgoTextVQA">[Dataset]</a>
              <br>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:280;height:130" src="images/vitxtgqa.png" alt="vitxtgqa">
            </td>
            <td width="70%" valign="center" style="line-height: 1.5;">
              <papertitle>Scene-Text Grounding for Text-Based Video Question Answering</papertitle>
              <br>
              <strong>Sheng Zhou</strong>, 
                Junbin Xiao, 
                Xun Yang, 
                Peipei Song, 
                Dan Guo, 
                Angela Yao, 
                Meng Wang, 
                Tat-Seng Chua.
              <br>
              <strong>Arxiv'24</strong> 
              <a href="https://arxiv.org/abs/2409.14319">[arXiv]</a>
              <a href="https://github.com/zhousheng97/ViTXT-GQA">[Code]</a>
              <a href="https://github.com/zhousheng97/ViTXT-GQA">[Dataset]</a>
              <br>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:280;height:130" src="images/gpin.png" alt="gpin">
            </td>
            <td width="70%" valign="center" style="line-height: 1.5;">
              <papertitle>Graph Pooling Inference Network for Text-based VQA</papertitle>
              <br>
              <strong>Sheng Zhou</strong>, 
                Dan Guo, 
                Xun Yang, 
                Jianfeng Dong, 
                Meng Wang.
              <br>
              <strong style="color: red;">TOMM'24</strong>
              <a href="https://dl.acm.org/doi/10.1145/3634918">[Paper]</a>
              <a href="https://github.com/zhousheng97/GPIN">[Code]</a>
              <br>
            </td>
          </tr>

                
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:280;height:130" src="images/ssgn.png" alt="ssgn">
            </td>
            <td width="75%" valign="center" style="line-height: 1.5;">
              <papertitle>Exploring Sparse Spatial Relation in Graph Inference for Text-Based VQA</papertitle>
              <br>
              <strong>Sheng Zhou</strong>, Dan Guo, Jia Li, Xun Yang, Meng Wang.
              <br>
              <strong style="color: red;">TIP'23</strong>
              <a href="https://ieeexplore.ieee.org/document/10241306">[Paper]</a>
              <a href="https://github.com/zhousheng97/SSGN">[Code]</a>
              <br>
            </td>
          </tr>
              

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Honors and Awards</heading>
              <p>
                
                <li style="margin: 5px;"> 
                  [2025.02] Tat-Seng Chua Scholarship
                </li>
                <li style="margin: 5px;"> 
                  [2022 - 2025] First Class Academic Scholarship (three times)
                </li>
                <li style="margin: 5px;"> 
                  [2020 - 2022] Second Class Academic Scholarship (two times)
                </li>
                <li style="margin: 5px;"> 
                  [2020] Outstanding Graduate of Innovation and Entrepreneurship in Hunan Province 
                </li>
                <li style="margin: 5px;"> 
                  [2016 - 2019] National Encouragement Scholarship (three times)
                </li>                
              </p>
            </td>
          </tr>
        </tbody></table>

      

        <p><center>
          <a href="https://clustrmaps.com/site/1c4rm" title="ClustrMaps">
          <img src="//www.clustrmaps.com/map_v2.png?d=Icn5np1Ziy1nrdxuhNF243TFUnolCVjDuXNjTMrnhOg&cl=ffffff" 
         width="300" height="200" />
          </a>
        </center></p>
      </td>
    </tr>
  </table>
 
</body>

</html>
